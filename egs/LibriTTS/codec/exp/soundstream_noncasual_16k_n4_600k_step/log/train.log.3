/home/users/ntu/ccdshyzh/miniconda3/envs/haoyang/bin/python /home/users/ntu/ccdshyzh/FunCodec/funcodec/bin/codec_train.py --gpu_id 3 --use_preprocessor true --train_data_path_and_name_and_type ./../../../../scratch/dump/LibriTTS/train/wav.scp,speech,kaldi_ark --train_shape_file ./exp/LibriTTS_states/train/speech_shape --valid_data_path_and_name_and_type ./../../../../scratch/dump/LibriTTS/dev/wav.scp,speech,kaldi_ark --valid_shape_file ./exp/LibriTTS_states/dev/speech_shape --ignore_init_mismatch true --resume true --output_dir ./exp/soundstream_noncasual_16k_n4_600k_step --config conf/soundstream_noncasual_16k_n4_600k_step.yaml --ngpu 4 --num_worker_count 1 --multiprocessing_distributed true --dist_init_method file:///home/users/ntu/ccdshyzh/FunCodec/egs/LibriTTS/codec/exp/soundstream_noncasual_16k_n4_600k_step/ddp_init --dist_world_size 4 --dist_rank 3 --local_rank 3
[W1019 01:10:59.212536787 Utils.hpp:164] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
/home/users/ntu/ccdshyzh/miniconda3/envs/haoyang/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
x1000c1s2b0n1:1730439:1730439 [3] NCCL INFO cudaDriverVersion 12020
x1000c1s2b0n1:1730439:1730439 [3] NCCL INFO Bootstrap : Using hsn0:10.150.0.43<0>
x1000c1s2b0n1:1730439:1730439 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
x1000c1s2b0n1:1730439:1730439 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin symbol (>= v5). ncclNetPlugin symbols v4 and lower are not supported.
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE [RO]; OOB hsn0:10.150.0.43<0>
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Using non-device net plugin version 0
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Using network IB
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO comm 0xa80dbf0 rank 3 nranks 4 cudaDev 3 nvmlDev 2 busId 81000 commId 0x2053aed68d6f064e - Init START
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,ffff0000
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO NVLS multicast support is not available on dev 3
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO comm 0xa80dbf0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->1 [5] -1/-1/-1->3->1 [6] -1/-1/-1->3->1 [7] -1/-1/-1->3->1 [8] 2/-1/-1->3->0 [9] 2/-1/-1->3->0 [10] 2/-1/-1->3->0 [11] 2/-1/-1->3->0 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->1 [17] -1/-1/-1->3->1 [18] -1/-1/-1->3->1 [19] -1/-1/-1->3->1 [20] 2/-1/-1->3->0 [21] 2/-1/-1->3->0 [22] 2/-1/-1->3->0 [23] 2/-1/-1->3->0
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO P2P Chunksize set to 524288
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 00/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 03/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 06/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 09/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 12/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 15/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 18/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 21/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 02/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 04/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 08/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 10/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 14/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 16/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 20/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 22/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 01/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 05/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 07/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 11/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 13/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 17/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 19/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 23/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Connected all rings
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 08/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 10/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 11/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 20/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 22/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 23/0 : 3[2] -> 0[1] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 05/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 06/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 07/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 17/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 18/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 19/0 : 3[2] -> 1[3] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 00/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 02/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 03/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 08/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 09/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 10/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 12/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 14/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 15/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 20/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 21/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Channel 22/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO Connected all trees
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO 24 coll channels, 0 collnet channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
x1000c1s2b0n1:1730439:1731407 [3] NCCL INFO comm 0xa80dbf0 rank 3 nranks 4 cudaDev 3 nvmlDev 2 busId 81000 commId 0x2053aed68d6f064e - Init COMPLETE
[rank3]:[W1019 01:11:15.759109673 Utils.hpp:110] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
Number of CUDA devices: 4
Device 0: NVIDIA A100-SXM4-40GB
  Compute Capability: 8.0
  Total Memory: 39.56 GB
Device 1: NVIDIA A100-SXM4-40GB
  Compute Capability: 8.0
  Total Memory: 39.56 GB
Device 2: NVIDIA A100-SXM4-40GB
  Compute Capability: 8.0
  Total Memory: 39.56 GB
Device 3: NVIDIA A100-SXM4-40GB
  Compute Capability: 8.0
  Total Memory: 39.56 GB
/home/users/ntu/ccdshyzh/FunCodec/funcodec/train/gan_trainer.py:183: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(scaler is not None):
[rank3]:[W1019 01:11:27.862228435 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/home/users/ntu/ccdshyzh/miniconda3/envs/haoyang/lib/python3.9/site-packages/torch/functional.py:666: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.
Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
